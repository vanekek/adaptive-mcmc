{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import random\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from models.vaes import Base, LMCVAE\n",
    "from models.samplers import HMC, run_chain\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(N, eps0=None, d=2, sigma=1.):\n",
    "    seed_everything(42)\n",
    "    z = np.random.randn(N, d)\n",
    "    x = 2 * np.pi * (np.linalg.norm(z, axis=1, keepdims=True) + 2.) + np.random.randn(N, 1) * sigma\n",
    "    return x\n",
    "\n",
    "\n",
    "class Toy(Base):   \n",
    "    def joint_logdensity(self, use_true_decoder=None):\n",
    "        def density(z, x):\n",
    "            if (use_true_decoder is not None) and use_true_decoder:\n",
    "                x_reconst = self(z)\n",
    "            elif hasattr(self, 'use_cloned_decoder') and self.use_cloned_decoder:\n",
    "                x_reconst = self.cloned_decoder(z)\n",
    "            else:\n",
    "                x_reconst = self(z)\n",
    "            log_Pr = torch.distributions.Normal(loc=torch.tensor(0., device=x.device, dtype=torch.float32),\n",
    "                                    scale=torch.tensor(1., device=x.device, dtype=torch.float32)).log_prob(\n",
    "                    z).sum(-1)\n",
    "            \n",
    "            return torch.distributions.Normal(loc=2 * np.pi * (torch.sqrt(torch.sum(torch.pow(z, 2), dim=1, keepdim=True)) + 2) + 0. * self.decoder_net.aux,\n",
    "                                                  scale=self.decoder_net.sigma).log_prob(x).sum(-1) + log_Pr\n",
    "\n",
    "        return density\n",
    "    \n",
    "    \n",
    "class LMCVAE_Toy(LMCVAE, Toy):\n",
    "    def loss_function(self, sum_log_weights):\n",
    "        loss = super(LMCVAE_Toy, self).loss_function(sum_log_weights)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super(ToyDataset, self).__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        sample = torch.tensor(self.data[item], dtype=torch.float32, device=device)\n",
    "        return sample, -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyEncoder(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 2*d),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(2*d, 2*d),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    \n",
    "class ToyDecoder(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.aux = nn.Parameter(torch.tensor(0., dtype=torch.float32))\n",
    "        self.log_alpha = nn.Parameter(torch.tensor(0, device=device, dtype=torch.float32))\n",
    "        self.log_beta = nn.Parameter(torch.tensor(0., device=device, dtype=torch.float32))\n",
    "        self.sigma = sigma\n",
    "\n",
    "    @property\n",
    "    def alpha(self,):\n",
    "        return torch.exp(self.log_alpha)\n",
    "    \n",
    "    @property\n",
    "    def beta(self,):\n",
    "        return torch.exp(self.log_beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.alpha * (torch.sqrt(torch.sum(torch.pow(x, 2), dim=1, keepdim=True)) + self.beta) # + torch.randn_like(x[:, :1]) * self.sigma + 0. * self.aux\n",
    "    \n",
    "class TrueDecoder(nn.Module):\n",
    "    def __init__(self, d, sigma, eps=None):\n",
    "        super().__init__()\n",
    "        self.register_buffer('eps', torch.tensor(eps, dtype=torch.float32))\n",
    "        self.aux = nn.Parameter(torch.tensor(0., dtype=torch.float32))\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, z):\n",
    "        return 2 * np.pi * (torch.sqrt(torch.sum(torch.pow(z, 2), dim=1, keepdim=True)) + 2.) # + t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "d = 2\n",
    "sigma = 1.\n",
    "eps = 2 + np.random.randn(1, d)\n",
    "X_train = generate_dataset(N=N, eps0=eps, d=d, sigma=sigma)\n",
    "X_val = generate_dataset(N=N // 100, eps0=eps, d=d, sigma=sigma)\n",
    "train_dataset = ToyDataset(data=X_train)\n",
    "val_dataset = ToyDataset(data=X_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True,)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_enc_dec(model):\n",
    "    model.encoder_net = ToyEncoder(d=d)\n",
    "    model.decoder_net = TrueDecoder(d=d, eps=eps, sigma=sigma)\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)\n",
    "lmcvae = LMCVAE_Toy(shape=28, act_func=nn.LeakyReLU,\n",
    "            num_samples=1, hidden_dim=d,\n",
    "            net_type='conv', dataset='toy',\n",
    "            step_size=0.01, K=5, use_transforms=False, learnable_transitions=False, return_pre_alphas=True, use_score_matching=False,\n",
    "                      ula_skip_threshold=0.1, grad_skip_val=0., grad_clip_val=0., use_cloned_decoder=False, variance_sensitive_step=True,\n",
    "                     acceptance_rate_target=0.9, annealing_scheme='all_learnable')\n",
    "lmcvae = replace_enc_dec(lmcvae)\n",
    "lmcvae.name = 'LMCVAE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trainer(model):\n",
    "    seed_everything(42)\n",
    "    tb_logger = pl_loggers.TensorBoardLogger('lightning_logs/')\n",
    "    trainer = pl.Trainer(logger=tb_logger, fast_dev_run=False, max_epochs=31, automatic_optimization=True, )\n",
    "    trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_trainer(lmcvae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_leapfrogs = 5\n",
    "step_size = 0.01\n",
    "n_samples = 10000\n",
    "hmc = HMC(n_leapfrogs=n_leapfrogs, step_size=step_size, partial_ref=False, use_barker=False).to(device)\n",
    "X_item = torch.tensor([[2 * np.pi * (2. + 2.)]], device=device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Receive true posterior sample:\n",
    "target_density = lambda z, x: torch.distributions.Normal(loc=2 * np.pi * (torch.sqrt(torch.sum(torch.pow(z, 2), dim=1, keepdim=True)) + 2.), scale=sigma).log_prob(X_item.repeat(n_samples, 1)).sum(\n",
    "    -1) + torch.distributions.Normal(loc=torch.tensor(0., device=device, dtype=torch.float32),\n",
    "                                     scale=torch.tensor(1., device=device, dtype=torch.float32)).log_prob(z).sum(-1)\n",
    "\n",
    "true_posterior_samples = run_chain(kernel=hmc, z_init=torch.randn(n_samples, 2, device=device), target=target_density, return_trace=False, n_steps=1, burnin=500).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def receive_model_samples(model, X_item, stacked=False):\n",
    "    with torch.no_grad():\n",
    "        x = X_item\n",
    "        model_samples, mu, logvar = model.enc_rep(x=x, n_samples=n_samples)\n",
    "        if model.name in ['LMCVAE', 'AMCVAE']:\n",
    "            model_samples_init = model_samples\n",
    "            model_samples = model.run_transitions(z=model_samples, x=x.repeat(n_samples, 1), mu=mu, logvar=logvar)[0]\n",
    "        elif model.name in ['VAE_with_Flows']:\n",
    "            model_samples = model.Flow(model_samples)[0]\n",
    "    return model_samples.cpu().numpy()\n",
    "\n",
    "def plot_contours(model, X_item, stacked=False, graph=None):\n",
    "    x = X_item\n",
    "    with torch.no_grad():\n",
    "        model_samples, mu, logvar = model.enc_rep(x=x, n_samples=n_samples)\n",
    "        logprobs = torch.distributions.Normal(loc=mu, scale=torch.exp(0.5 * logvar)).log_prob(model_samples).sum(-1)\n",
    "#     plt.contour(model_samples[:, 0].cpu()[..., None], model_samples[:, 1].cpu()[..., None], logprobs.cpu()[..., None])\n",
    "    if graph is None:\n",
    "        plt.scatter(model_samples[:, 0].cpu(), model_samples[:, 1].cpu(),)\n",
    "    else:\n",
    "        graph.x = model_samples[:, 0].cpu()\n",
    "        graph.y = model_samples[:, 1].cpu()\n",
    "        graph.plot_joint(plt.scatter, marker='x', c='g', s=50, alpha=0.5)\n",
    "    \n",
    "\n",
    "def receive_posterior_samples(model, n_samples, stacked=False):\n",
    "    with torch.no_grad():\n",
    "        model_target_density = lambda z, x: model.joint_logdensity()(z=z, x=X_item.repeat(n_samples, 1))\n",
    "        model_posterior_samples = run_chain(kernel=hmc, z_init=torch.randn(n_samples, 2, device=device), target=model_target_density, return_trace=False, n_steps=1, burnin=500).cpu().numpy()\n",
    "    return model_posterior_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmcvae_sample = receive_model_samples(lmcvae, X_item)\n",
    "lmcvae_posterior_sample = receive_posterior_samples(lmcvae, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "# ========================================\n",
    "graph = sns.jointplot(x=true_posterior_samples[:, 0], y=true_posterior_samples[:, 1], color='r', alpha=0.5);\n",
    "# graph = sns.jointplot(x=vae_posterior_sample[:, 0], y=vae_posterior_sample[:, 1], color='r', alpha=0.5);\n",
    "# graph = sns.jointplot(x=iwae_posterior_sample[:, 0], y=iwae_posterior_sample[:, 1], color='r', alpha=0.5);\n",
    "# graph = sns.jointplot(x=lmcvae_posterior_sample[:, 0], y=lmcvae_posterior_sample[:, 1], color='r', alpha=0.5);\n",
    "# graph = sns.jointplot(x=amcvae_posterior_sample[:, 0], y=amcvae_posterior_sample[:, 1], color='r', alpha=0.5);\n",
    "# graph = sns.jointplot(x=flows_vae_posterior_sample[:, 0], y=flows_vae_posterior_sample[:, 1], color='r', alpha=0.5);\n",
    "\n",
    "graph.x = lmcvae_sample[:, 0]\n",
    "graph.y = lmcvae_sample[:, 1]\n",
    "graph.plot_joint(plt.scatter, marker='x', c='y', s=50, alpha=0.1);\n",
    "\n",
    "# plt.xlim(-5., 5.)\n",
    "# plt.ylim(-5., 5.)\n",
    "# plt.axis('equal');\n",
    "\n",
    "\n",
    "# plot_contours(model=stacked_vae, X_item=X_item, stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(samples, x_limits, y_limits, title=None, gamma=0.75, name='default.png'):\n",
    "    import matplotlib.colors as mcolors\n",
    "    from scipy.stats import kde\n",
    "    plt.close()\n",
    "    plt.figure(figsize=(5, 5), dpi=300)\n",
    "    \n",
    "    x = samples[:, 0]\n",
    "    y = samples[:, 1]\n",
    "    nbins = 300\n",
    "    k = kde.gaussian_kde([x,y], bw_method=0.1)\n",
    "    xi, yi = np.mgrid[x_limits[0]:x_limits[1]:nbins*1j, y_limits[0]:y_limits[1]:nbins*1j]\n",
    "    zi = k(np.vstack([xi.flatten(), yi.flatten()]))\n",
    "\n",
    "    # Make the plot\n",
    "    plt.pcolormesh(xi, yi, zi.reshape(xi.shape))\n",
    "    plt.axis('off')\n",
    "    plt.xlim((x_limits[0], x_limits[1]))\n",
    "    plt.ylim((y_limits[0], y_limits[1]))\n",
    "#     plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(name, format='png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(true_posterior_samples, (-4, 4), (-4, 4), title=None, gamma=0.5, name='true_posterior.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(lmcvae_sample, (-4, 4), (-4, 4), title=None, gamma=0.5, name='lmcvae.png')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
